# מיטל סבטן | 206391807
[[#שאלה 1]]
[[#שאלה 2]]
[[#שאלה 3]]

---
# שאלה 1
נשתמש בשיטת הנראות המרבית על מנת לאמוד את $\mu_{1}$ ו-$\mu_{2}$.
ראשית, נשתמש בפונקציית ההתפלגות של משתנה מיקרי נורמלי ונגדיר :
$$
\begin{gather}
L_1 = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(X_1 - \mu_1)^2}{2\sigma^2}}\cdot \dots \cdot \dfrac{1}{\sqrt{ 2\pi \sigma^2 }}e^{-\frac{(X_n - \mu_1)^2}{2\sigma^2}}\\
\implies L_{1}=(\dfrac{1}{\sqrt{ 2\pi \sigma^2 }})^ne^{\frac{1}{2\sigma^2}\sum_{i=1}^{n}(X_{i}-\mu_{1})^2}
\end{gather}

$$
ובאותו האופן:
$$
\begin{gather}
L_2 = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(Y_1 - \mu_2)^2}{2\sigma^2}}\cdot \dots \cdot \dfrac{1}{\sqrt{ 2\pi \sigma^2 }}e^{-\frac{(Y_n - \mu_2)^2}{2\sigma^2}}\\
\implies L_{2}=(\dfrac{1}{\sqrt{ 2\pi \sigma^2 }})^ne^{\frac{1}{2\sigma^2}\sum_{i=1}^{n}(Y_{i}-\mu_{2})^2}
\end{gather}
$$
$$
\begin{gather}
L_3 = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(W_1 - (\mu_{1}+\mu_2))^2}{2\sigma^2}}\cdot \dots \cdot \dfrac{1}{\sqrt{ 2\pi \sigma^2 }}e^{-\frac{(W_n -(\mu_{1}+ \mu_2)^2}{2\sigma^2}}\\
\implies L_{3}=(\dfrac{1}{\sqrt{ 2\pi \sigma^2 }})^ne^{\frac{1}{2\sigma^2}\sum_{i=1}^{n}(W_{i}-(\mu_{1}+\mu_{2})^2}
\end{gather}
$$
נגדיר את הפונקציה הבאה, נגזור אותה לפי $\mu_{1}$ ונחפש נקודת מקסימום: 
$$
\begin{gather}
L=L_{1}\cdot L_{2} \cdot L_{3}= (\dfrac{1}{\sqrt{ 2\pi \sigma^2 }})^{3n}\cdot e^{\frac{-1}{2\sigma^2}\sum_{i=1}^n((X_{i}-\mu_{1})^2+(Y_{i}-\mu_{2})^2+(W_{i}-(\mu_{1}+\mu_{2}))^2)}\\ 
\implies \ln(L)=3n\ln\left( \dfrac{1}{\sqrt{ 2\pi \sigma^2 }}\right)-\dfrac{1}{2\sigma^2}\sum_{i=1}^n\left((X_{i}-\mu_{1})^2+(Y_{i}-\mu_{2})^2+(W_{i}-(\mu_{1}+\mu_{2}))^2\right)\\
\implies 0=\dfrac{{\partial \ln(L)}}{\partial \mu_{1}}=-\dfrac{1}{2\sigma^2}\cdot \sum_{i=1}^n(-2(X_{i}-\mu_{1})-2(W_{i}-(\mu_{1}+\mu_{2})))\\
\implies \dfrac{1}{\sigma^2}\sum_{i=1}^n(X_{i}+W_{i}-2\mu_{1}-\mu_{2})=0\\
2n\mu_{1}+n\mu_{2}=\sum_{i=1}^nX_{i}+\sum_{i=1}^n
W_{i}\end{gather}
$$
נוודא שאכן מצאנו נקודת מקסימום: 
$$
\begin{gather}
\dfrac{{\partial^2 \ln(L)}}{\partial \mu_{1}^2}= \dfrac{1}{\sigma^2}\sum_{i=1}^n-2<0 \implies max \ value
\end{gather}
$$
כעת נגזור את L לפי $\mu_{2}$ ונחפש נקודת מקסימום:
$$
\begin{gather}
 0=\dfrac{{\partial \ln(L)}}{\partial \mu_{2}}=-\dfrac{1}{2\sigma^2}\cdot \sum_{i=1}^n(-2(Y_{i}-\mu_{2})-2(W_{i}-(\mu_{1}+\mu_{2})))\\
\implies \dfrac{1}{\sigma^2}\sum_{i=1}^n(Y_{i}+W_{i}-2\mu_{2}-\mu_{1})=0\\
2n\mu_{2}+n\mu_{1}=\sum_{i=1}^nY_{i}+\sum_{i=1}^n
W_{i}\end{gather}
$$
שוב נוודא שאכן מדובר בנקודת מקסימום:
$$
\begin{gather}
\dfrac{{\partial^2 \ln(L)}}{\partial \mu_{2}^2}= \dfrac{1}{\sigma^2}\sum_{i=1}^n-2<0 \implies max \ value
\end{gather}
$$
קיבלנו 2 משוואות עם 2 נעלמים. נציב האחת בשנייה ונקבל:
$$
\begin{gather}
\mu_{1}= \dfrac{{2\sum_{i=1}^nX_{i}-\sum_{i=1}^nY_{i}+\sum_{i=1}^nW_{i}}}{3n}\\
\mu_{2}= \dfrac{{-\sum_{i=1}^nX_{i}+2\sum_{i=1}^nY_{i}+\sum_{i=1}^nW_{i}}}{3n}\\
\end{gather}
$$
---
# שאלה 2

א. נתון כי $X \sim U[0, \theta]$ נשתמש בשיטת הנראות המרבית על מנת למצוא אומד ל- $\theta$. נגדיר:
$$
\begin{gather}
L(\theta|x_{1},\dots,x_{n})= \prod_{i=1}^nf(x_{i}|\theta)\implies\\
L(\theta|x_{1},\dots,x_{n})=\dfrac{1}{\theta}\cdot \dots \cdot \underbrace{ \dfrac{1}{\theta} }_{ n }= \left( \dfrac{1}{\theta}\right)^n
\end{gather}
$$
לפי ההגדרה, אומד בשיטת הנראות המרבית מוגדר כערך של $\theta$ המביא לערכה המקסימלי של $L(\theta)$. 
$$
L(\hat{\theta}) = \max_{\theta \in \Theta} L(\theta)
$$
לכן, עבור שאלה זו $L(\theta)$ יקבל ערך מקסימלי עבור $\theta$ מינימלי (n מספר שלם וחיובי). נשים לב ש-$\theta$ מגדיר את הגבול העליון של המדידות ולכן, כל המדידות $x_{i}$ חייבות להיות בתחום $[0,\theta]$, כלומר, האומדן לקבלת $\theta$  מינימלי יהיה המדידה המקסימלית $\hat{\theta}=X_{max}$. 

ב. נמצא אומד לפי שיטת המומנים על סמך המומנט ה-1:
$$
\begin{gather}
\mu_{1}=E[X]= \int_{0}^\theta  \dfrac{1}{\theta}xdx= \dfrac{\theta}{2}
\end{gather}
$$
ניתן לבודד את $\theta$ ולמצוא אותה כפונקציה של $\mu_{1}$:
$$
\implies \theta=2\mu_{1}\\
$$
עכשיו נאמוד את הפרמטר על ידי החלפת המומנטים במומנט המדגמי המתאים:
$$
\hat{\theta_{\mu_{1}}}=f(M_{1})=2M_{1}=2 \bar{X}= \dfrac{2}{n} \sum_{i=1}^n x_{i}
$$
ג. נוכל לאמוד את $\theta$ גם באמצעות המומנט השני: 
$$
\begin{gather}
\mu_{2}= E[X^2]= Var(X)+(E[X])^2= \dfrac{(\theta-0)^2}{12}+\left(\dfrac{\theta}{2}\right)^2= \dfrac{\theta^2}{3}\implies \theta=\sqrt{ 3\mu_{2} }\\
\hat{\theta_{\mu_{2}}}=g(M_{2})= \sqrt{ 3M_{2} }=\sqrt{ \dfrac{3}{n} \sum_{i=1}^n x_{i}^2}
\end{gather}
$$
---
# שאלה 3
א. נמצא משערך נראות מרבית ל-$\theta$, נגדיר:
$$
\begin{gather}
L(\theta)= \dfrac{1}{\theta}e^\frac{-(x_{1}-3)}{\theta}\cdot \dots \cdot \underbrace{ \dfrac{1}{\theta} e^\frac{-(x_{n}-3)}{\theta} }_{ n }= (\dfrac{1}{\theta})^n e^{\frac{-1}{\theta} \sum _{i=1}^n (x_{i}-3)}\\
\implies \ln(L(\theta))= -n\ln(\theta)-\dfrac{1}{\theta}\sum_{i=1}^n (x_{i}-3)\\
0= \dfrac{{\partial \ln(L(\theta))}}{\partial \theta}= -\dfrac{n}{\theta}+ \dfrac{1}{\theta^2}\sum_{i=1}^n (x_{i}-3)\\
\implies \theta= \dfrac{1}{n} \sum_{i=1}^n (x_{i}-3)= \bar{X}-3
\end{gather}
$$
נוודא שאכן קיבלנו נקודת מקסימום: 
$$
\begin{gather}
\dfrac{\partial^2\ln(L)}{\partial \theta^2}= \dfrac{n}{\theta^2}-\dfrac{2}{\theta^3}\sum_{i=1}^n(x_{i}-3)
\end{gather}
$$
נציב את הנקודה שקיבלנו ונוודא שאכן מדובר בנקודת מקסימום: 
$$
\begin{gather}
\dfrac{n}{\left({\dfrac{1}{n} \sum_{i=1}^n (x_{i}-3)}\right)^2}-\dfrac{2\sum_{i=1}^n(x_{i}-3)}{{\left(\dfrac{1}{n} \sum_{i=1}^n (x_{i}-3) \right)}^3}= \\
\dfrac{n}{(\bar{X}-3)^2} -\dfrac{2n}{(\bar{X}-3)^2}= -\dfrac{n}{(\bar{X}-3)^2}<0
\end{gather}
$$
ב. שיטת MAP מוגדרת באופן הבא: 
$$
\theta_{\text{MAX}} = \arg\max \left[ \ln P(x_1, \ldots, x_n \mid \theta) + \ln P(\theta) \right]
$$
אם נבחן את הביטוי שקיבלנו לאחר גזירה עבור $\theta$ נשים לב כי האיבר הימני לא קיים בביטוי מהסעיף הקודם, משמע $\dfrac{{\partial \ln(P(\theta))}}{\partial \theta}= 0$  ולכן $P(\theta)=const$. כלומר, ההנחה שנלקחה עבור הפרמטר $\theta$ על מנת לשערך בשיטת MAP היא ש-$\theta$ מתפלג בצורה אחידה ולכן בעל פונקציית צפיפות קבועה מה שיאפס את האיבר הימני בעת הגזירה.

ג. $\theta \sim \exp(\lambda=3)$, נכתוב את המשוואה לחישוב אומד הנראות המירבית ל-$\theta$ בהתחשב בנתון החדש. 
$$
\begin{gather}
\theta_{\text{MAX}} = \arg\max \left[ \ln P(x_1, \ldots, x_n \mid \theta) + \ln P(\theta) \right]\implies \\ 
= -n\ln(\theta) -\dfrac{1}{\theta} \sum_{i=1}^n (x_{i}-3) +\ln(3)-3\theta
\end{gather}
$$
לבסוף, יש לגזור את הפונקציה לפי $\theta$, להשוות ל-0 ולוודא שאכן התקבלה נקודת מקסימום. 
